import streamlit as st
import os
import re

# --- 1. ç³»çµ±è¨­å®š ---
st.set_page_config(page_title="YG äººé¡åœ–æ–‡ç»æª¢ç´¢ç³»çµ±", layout="wide")

# --- 2. æ ¸å¿ƒæœå°‹å¼•æ“ (ç´” Python è™•ç†) ---
@st.cache_data(show_spinner=False)
def get_knowledge_base():
    base_path = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(base_path, "knowledge_base.txt")
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            # è®€å–ä¸¦ä»¥ã€Œé›™æ›è¡Œã€åˆ‡åˆ†æ®µè½ï¼Œé€™é€šå¸¸æ˜¯æ›¸ä¸­çŸ¥è­˜é»çš„è‡ªç„¶åˆ†éš”
            content = f.read()
            return content.split('\n\n')
    return []

def keyword_search(paragraphs, keywords):
    """
    ç²¾æº–æœå°‹åŒ…å«é—œéµå­—çš„æ®µè½ï¼Œä¸¦å»é™¤é‡è¤‡
    """
    results = []
    for para in paragraphs:
        # åªè¦æ®µè½ä¸­åŒ…å«ä»»ä½•ä¸€å€‹é—œéµå­—ï¼Œå°±æŠ“å‡ºä¾†
        if any(key.strip() in para for key in keywords if key.strip()):
            results.append(para.strip())
    # å»é™¤é‡è¤‡æ®µè½ä¸¦ä¿æŒé †åº
    return list(dict.fromkeys(results))

# é è¼‰å…¥æ–‡ç»
all_paragraphs = get_knowledge_base()

# --- 3. ä¸»ç•«é¢ä»‹é¢ ---
st.title("ğŸ›¡ï¸ äººé¡åœ–åŸå» è¨­å®šï¼šæ–‡ç»è‡ªå‹•æª¢ç´¢ç³»çµ±")
st.markdown("---")

if not all_paragraphs:
    st.error("âŒ æ‰¾ä¸åˆ° `knowledge_base.txt`ï¼Œè«‹ç¢ºèªæª”æ¡ˆå·²ä¸Šå‚³è‡³ GitHub æ ¹ç›®éŒ„ã€‚")
else:
    # ä»‹é¢ä½ˆå±€
    with st.container():
        st.subheader("ğŸ“Š è¼¸å…¥æ‚¨çš„æ•¸æ“š")
        c1, c2, c3 = st.columns([2, 2, 3])
        
        with c1:
            u_type = st.selectbox("1. æ‚¨çš„é¡å‹", ["æŠ•å°„è€…", "ç”Ÿç”¢è€…", "é¡¯ç¤ºç”Ÿç”¢è€…", "é¡¯ç¤ºè€…", "åæ˜ è€…"])
            u_auth = st.text_input("2. å…§åœ¨æ¬Šå¨", value="ç›´è¦º")
            
        with c2:
            u_ch = st.text_input("3. é€šé“æ•¸å­— (ç”¨ç©ºæ ¼æˆ–é€—è™Ÿåˆ†é–‹)", placeholder="10-20, 7-31")
            u_gt = st.text_input("4. é–˜é–€æ•¸å­— (ç”¨é€—è™Ÿåˆ†é–‹)", placeholder="31, 41, 10...")
            
        with c3:
            st.info("ğŸ’¡ **ç³»çµ±èªªæ˜**ï¼š\næœ¬ç³»çµ±å°‡ç›´æ¥æª¢ç´¢æ‚¨æä¾›çš„ 7 æœ¬äººé¡åœ–æ–‡ç»ã€‚ä¸ä½¿ç”¨ AI APIï¼Œå› æ­¤ä¸å—æµé‡é™åˆ¶ã€‚å»ºè­°é‡å°ç‰¹å®šé–˜é–€æŸ¥çœ‹åŸæ–‡è§£èªªã€‚")

    if st.button("ğŸš€ å•Ÿå‹•æ–‡ç»å…¨æ–¹ä½æª¢ç´¢", use_container_width=True):
        # æ•´ç†é—œéµå­—
        # è™•ç†é€šé“ï¼šæŠŠ 10-20 æ‹†æˆ 10, 20
        ch_list = re.split(r'[,\s-]+', u_ch) if u_ch else []
        # è™•ç†é–˜é–€ï¼šæ‹†åˆ†æ•¸å­—
        gt_list = re.split(r'[,\s]+', u_gt) if u_gt else []
        
        # å»ºç«‹æœå°‹æ¸…å–® (é¡å‹ã€æ¬Šå¨ã€é€šé“ã€é–˜é–€)
        search_terms = [u_type, u_auth] + ch_list + gt_list
        search_terms = [t for t in search_terms if t] # éæ¿¾ç©ºå€¼

        st.success(f"ğŸ” æ­£åœ¨é‡å°é—œéµå­—ï¼š{', '.join(search_terms)} é€²è¡Œæ–‡ç»æ¯”å°...")

        # åŸ·è¡Œæœå°‹
        found_content = keyword_search(all_paragraphs, search_terms)

        if found_content:
            # ä½¿ç”¨ Tabs å‘ˆç¾ä¸åŒåˆ†é¡ï¼Œç•«é¢æ›´æ•´æ½”
            tab1, tab2 = st.tabs(["ğŸ“œ ç›¸é—œæ–‡ç»åŸæ–‡", "ğŸ“Œ é—œéµå­—é€ŸæŸ¥"])
            
            with tab1:
                st.write(f"å…±æ‰¾åˆ° {len(found_content)} æ®µç›¸é—œæ–‡ç»ç‰‡æ®µï¼š")
                for i, text in enumerate(found_content):
                    with st.expander(f"æ–‡ç»ç‰‡æ®µ {i+1}", expanded=(i==0)):
                        st.markdown(text)
            
            with tab2:
                st.write("æ‚¨å¯ä»¥åˆ©ç”¨ç€è¦½å™¨æœå°‹ (Ctrl+F) åœ¨ä¸‹æ–¹å¿«é€Ÿå®šä½ï¼š")
                full_result = "\n\n---\n\n".join(found_content)
                st.text_area("æ‰€æœ‰çµæœå…¨æ–‡ï¼š", value=full_result, height=500)
        else:
            st.warning("âš ï¸ åœ¨æ–‡ç»ä¸­æ‰¾ä¸åˆ°èˆ‡æ‚¨è¼¸å…¥æ•¸æ“šå®Œå…¨åŒ¹é…çš„æ–‡å­—ï¼Œè«‹å˜—è©¦ç°¡åŒ–é—œéµå­—ï¼ˆä¾‹å¦‚åªè¼¸å…¥æ•¸å­—ï¼‰ã€‚")

st.divider()
st.caption("è³‡æ–™ä¾†æºï¼šææ™é§’ (YG) å°ˆå±¬äººé¡åœ–å¤§è³‡æ–™åº«ã€‚æœ¬ç³»çµ±åƒ…æä¾›æ–‡ç»æª¢ç´¢ï¼Œä¸ä»£è¡¨é†«ç™‚æˆ–è·æ¥­è¨ºæ–·å»ºè­°ã€‚")
